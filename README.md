# Scan_Blacklink

## English:

Scan the source code of the website application to see if there is a third-party black chain

Most website systems are susceptible to blacklink injection. Analysis indicates that this issue most likely falls into one of the following two categories:

1. It's possible that developers cut corners during website development by directly copying and pasting open-source framework code containing blacklink code into certain parts of the website's source code. This leads to the website being flagged as having blacklinks injected by attackers, when in reality it's a developer error. ã€This falls under local code invocation issuesã€‘

2. It could be that during website development, the developer directly invoked certain insecure third-party resources remotely. These third-party resources themselves contained blacklink code, leading to the website being flagged as having blacklinks injected by attackers. In reality, this is a developer error. ã€This falls under remote resource invocation issuesã€‘


Therefore, I referenced the latest TLD list provided by the IANA Root Zone to accommodate most top-level domain suffixes on the internet. By employing multiple regular expressions, I ensured that third-party domain names within the local source code were extracted.

ğŸ’¡Here are three scripts: Scan_Blacklink.py, Get_Pagesource.py, and Download_har.py.


### 1ã€Scan_Blacklink:

Usage: python3 scan_blacklink.py [-h] [-d DIRECTORY] [-b BASE_DOMAIN] [-o OUTPUT] [--no-timestamp] [-r] [-nr] [-e EXTENSIONS | -a] [-t THREADS] [-bl BLACKLIST] [--probe]
               [--probe-timeout PROBE_TIMEOUT] [--probe-workers PROBE_WORKERS]

Extract hidden links and external links from all source code files in the directory (supports multi-threading acceleration and HTTP probing for suspected black links)

```
options:
  -h, --help            show this help message and exit
  -d DIRECTORY, --directory DIRECTORY
                        Directory path to process, defaults to current directory
  -b BASE_DOMAIN, --base-domain BASE_DOMAIN
                        Base domain used to determine external links (e.g., https://example.com)
  -o OUTPUT, --output OUTPUT
                        Output file path. Default: automatically generates filenames with timestamps
  --no-timestamp        Output filenames without timestamps
  -r, --recursive       Recursively process subdirectories (enabled by default)
  -nr, --no-recursive   Do not recursively process subdirectories
  -e EXTENSIONS, --extensions EXTENSIONS
                        Comma-separated file extensions (e.g., html,php,js) or (.html,.php,.js)
  -a, --all             Scan all files (no extension restrictions, may be slower)
  -t THREADS, --threads THREADS
                        Number of threads (default: 4, recommended range: 1-16)
  -bl BLACKLIST, --blacklist BLACKLIST
                        Blacklist domain/keyword list file, one entry per line, supports substring matching (e.g., test.xyz, ppp.qr, etc.), appended to built-in keywords
  --probe               Perform HTTP probes on detected suspected blacklinks/hidden links (requires requests library installation)
  --probe-timeout PROBE_TIMEOUT
                        HTTP probe timeout (seconds), default 5 seconds
  --probe-workers PROBE_WORKERS
                        Number of concurrent HTTP probe threads, default 8


Example:
  scan_blacklink.py -d /path/to/dir                  # Scan specified directory
  scan_blacklink.py --all                            # Scan all files (no extension restrictions)
  scan_blacklink.py -e html,php,js                   # Scan only specified extensions
  scan_blacklink.py -t 8                             # Use 8 threads for acceleration
  scan_blacklink.py -b https://example.com           # Specify root domain for external link detection
  scan_blacklink.py -bl blacklist.txt                # Append blacklink domain/keyword list to built-in keywords
  scan_blacklink.py --probe                          # Perform HTTP probe on suspected blacklinks

Common parameters, such as:
python3 scan_blacklink.py -d /path/to/dir --probe

During the process of scanning source code, extract corresponding domain names while simultaneously performing HTTP probes to obtain response information.
```

Dec 17 update:

Added script program execution recovery functionality; parameters remain unchanged.

<img width="1771" height="446" alt="image" src="https://github.com/user-attachments/assets/bf76bf29-b95b-4e11-a895-0fd325fcc8c7" />




### 2ã€Get_Pagesource:

This script's functionality involves using Selenium to simulate requests, then downloading the corresponding frontend page source code files locally. Simply define the target URL within the script code to enable automated crawling. Of course, you'll need to configure the proxy auto-switching mode to avoid having your IP blocked by WAF.

Usage: python3 Get_Pagesource.py


### 3ã€Download_har:

If you simply use the Get_Pagesource.py script to download the frontend page source files, some files may be missing. This is because many websites now employ asynchronous loading to enable faster access. Consequently, corresponding resource files are only loaded when you intentionally click a specific function or page during browsing. You can view detailed information about this loading process in the Network tab of your browser's console. Simultaneously, you can download the contents of the Network tab pageâ€”i.e., the currently loaded resource filesâ€”to your local machine as a har file. This file contains URLs pointing to all resource files. Simply executing the Download_har.py script will automatically download the corresponding resource files. The Download_har.py script was actually created to complement the Get_Pagesource.py script mentioned above. It enables the complete download of a target website's frontend source code, allowing you to even set it up locally. Of course, you'll need to configure the proxy auto-switching mode to avoid having your IP blocked by WAF.

Usage: python3 Download_har.py


## Simplified Chinese(ç®€ä½“ä¸­æ–‡):


æ‰«æç½‘ç«™åº”ç”¨ç¨‹åºçš„æºä»£ç ï¼ŒæŸ¥çœ‹æ˜¯å¦å­˜åœ¨ç¬¬ä¸‰æ–¹é»‘é“¾

å¤§éƒ¨åˆ†ç½‘ç«™ç³»ç»Ÿå­˜åœ¨è¢«æŒ‚é»‘é“¾çš„ç°è±¡ï¼Œç»è¿‡åˆ†æå¤§æ¦‚ç‡æ˜¯å±äºä¸‹é¢çš„è¿™ä¸¤ç§æƒ…å†µï¼š

1ã€æœ‰å¯èƒ½æ˜¯å¼€å‘äººå‘˜åœ¨å¼€å‘ç½‘ç«™çš„æ—¶å€™å·æ‡’äº†ï¼Œç›´æ¥æŠŠä¸€äº›åŒ…å«äº†é»‘é“¾ä»£ç çš„å¼€æºæ¡†æ¶ä»£ç å¤åˆ¶ç²˜è´´åˆ°ç½‘ç«™æºç çš„æŸäº›åœ°æ–¹ï¼Œå¯¼è‡´ç½‘ç«™è¢«è®¤ä¸ºæ˜¯è¢«æ”»å‡»è€…æŒ‚é»‘é“¾ï¼Œä½†å®é™…ä¸Šæ˜¯å¼€å‘äººå‘˜çš„å¤±è¯¯ã€‚ã€è¿™æ˜¯å±äºæœ¬åœ°ä»£ç è°ƒç”¨é—®é¢˜ã€‘

2ã€æœ‰å¯èƒ½æ˜¯å¼€å‘äººå‘˜åœ¨å¼€å‘ç½‘ç«™çš„æ—¶å€™ï¼Œç›´æ¥è¿œç¨‹è°ƒç”¨äº†æŸäº›ä¸å®‰å…¨çš„ç¬¬ä¸‰æ–¹èµ„æºï¼Œè¿™äº›ç¬¬ä¸‰æ–¹èµ„æºæœ¬èº«å°±åŒ…å«äº†é»‘é“¾ä»£ç ï¼Œå¯¼è‡´ç½‘ç«™è¢«è®¤ä¸ºæ˜¯è¢«æ”»å‡»è€…æŒ‚é»‘é“¾ï¼Œä½†å®é™…ä¸Šæ˜¯å¼€å‘äººå‘˜çš„å¤±è¯¯ã€‚ã€è¿™æ˜¯å±äºè¿œç¨‹èµ„æºè°ƒç”¨é—®é¢˜ã€‘

å› æ­¤ï¼Œæˆ‘å‚è€ƒäº†IANAæ ¹åŸŸæä¾›çš„æœ€æ–°é¡¶çº§åŸŸååˆ—è¡¨ï¼Œä»¥è¦†ç›–äº’è”ç½‘ä¸Šç»å¤§å¤šæ•°é¡¶çº§åŸŸååç¼€ï¼Œå¹¶é€šè¿‡è¿ç”¨å¤šé‡æ­£åˆ™è¡¨è¾¾å¼ï¼Œç¡®ä¿ä»æœ¬åœ°æºä»£ç ä¸­æå–å‡ºæ‰€æœ‰ç¬¬ä¸‰æ–¹åŸŸåã€‚

ğŸ’¡è¿™é‡Œæœ‰ä¸‰ä¸ªè„šæœ¬ï¼Œåˆ†åˆ«æ˜¯Scan_Blacklink.pyã€Get_Pagesource.pyã€Download_har.py



### 1ã€Scan_Blacklink:

usage: scan_blacklink.py [-h] [-d DIRECTORY] [-b BASE_DOMAIN] [-o OUTPUT] [--no-timestamp] [-r] [-nr] [-e EXTENSIONS | -a] [-t THREADS] [-bl BLACKLIST] [--probe]
                         [--probe-timeout PROBE_TIMEOUT] [--probe-workers PROBE_WORKERS]

æå–ç›®å½•ä¸­æ‰€æœ‰æºä»£ç æ–‡ä»¶çš„æš—é“¾å’Œå¤–é“¾åœ°å€ï¼ˆæ”¯æŒå¤šçº¿ç¨‹åŠ é€Ÿï¼Œå¹¶å¯å¯¹ç–‘ä¼¼é»‘é“¾è¿›è¡ŒHTTPæ¢æµ‹ï¼‰

```
options:
  -h, --help            show this help message and exit
  -d DIRECTORY, --directory DIRECTORY
                        è¦å¤„ç†çš„ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
  -b BASE_DOMAIN, --base-domain BASE_DOMAIN
                        åŸºç¡€åŸŸåï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºå¤–é“¾ï¼ˆå¦‚ https://example.comï¼‰
  -o OUTPUT, --output OUTPUT
                        è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
  --no-timestamp        è¾“å‡ºæ–‡ä»¶åä¸åŒ…å«æ—¶é—´æˆ³
  -r, --recursive       æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
  -nr, --no-recursive   ä¸é€’å½’å¤„ç†å­ç›®å½•
  -e EXTENSIONS, --extensions EXTENSIONS
                        é€—å·åˆ†éš”çš„æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚: html,php,jsï¼‰æˆ–ï¼ˆ.html,.php,.jsï¼‰
  -a, --all             æ‰«ææ‰€æœ‰æ–‡ä»¶ï¼ˆä¸é™æ‰©å±•åï¼Œå¯èƒ½è¾ƒæ…¢ï¼‰
  -t THREADS, --threads THREADS
                        çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ä¸º4ï¼Œå»ºè®®èŒƒå›´ï¼š1-16ï¼‰
  -bl BLACKLIST, --blacklist BLACKLIST
                        é»‘é“¾åŸŸå/å…³é”®å­—åˆ—è¡¨æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œæ”¯æŒå­ä¸²åŒ¹é…ï¼ˆå¦‚ï¼šceshi.xyzã€ppp.qr ç­‰ï¼‰ï¼Œä¼šåœ¨å†…ç½®å…³é”®è¯åŸºç¡€ä¸Šè¿½åŠ 
  --probe               å¯¹å‘½ä¸­çš„ç–‘ä¼¼é»‘é“¾/éšè—é“¾æ¥è¿›è¡ŒHTTPæ¢æµ‹ï¼ˆéœ€è¦å®‰è£…requestsåº“ï¼‰
  --probe-timeout PROBE_TIMEOUT
                        HTTPæ¢æµ‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
  --probe-workers PROBE_WORKERS
                        HTTPæ¢æµ‹å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤8

ç¤ºä¾‹:
  scan_blacklink.py -d /path/to/dir                  # æ‰«ææŒ‡å®šç›®å½•
  scan_blacklink.py --all                            # æ‰«ææ‰€æœ‰æ–‡ä»¶ï¼ˆä¸é™æ‰©å±•åï¼‰
  scan_blacklink.py -e html,php,js                   # åªæ‰«ææŒ‡å®šæ‰©å±•å
  scan_blacklink.py -t 8                             # ä½¿ç”¨8ä¸ªçº¿ç¨‹åŠ é€Ÿ
  scan_blacklink.py -b https://example.com           # æŒ‡å®šåŸºç¡€åŸŸåè¯†åˆ«å¤–é“¾
  scan_blacklink.py -bl blacklist.txt                # åœ¨å†…ç½®å…³é”®è¯åŸºç¡€ä¸Šè¿½åŠ é»‘é“¾åŸŸå/å…³é”®å­—åˆ—è¡¨
  scan_blacklink.py --probe                          # å¯¹ç–‘ä¼¼é»‘é“¾è¿›è¡ŒHTTPæ¢æµ‹

å¸¸ç”¨çš„å‚æ•°ï¼Œæ¯”å¦‚ï¼š
python3 scan_blacklink.py -d /path/to/dir --probe

åœ¨æ‰«ææºç çš„è¿‡ç¨‹ä¸­æå–å¯¹åº”çš„åŸŸåå¹¶åŒæ—¶è¿›è¡ŒHTTPæ¢æµ‹è·å–å“åº”ä¿¡æ¯
```

12æœˆ7æ—¥æ›´æ–°: 

æ–°å¢è„šæœ¬ç¨‹åºæ¢å¤æ‰§è¡ŒåŠŸèƒ½ï¼Œä½¿ç”¨å‚æ•°ä¾ç„¶ä¸å˜

<img width="1771" height="446" alt="image" src="https://github.com/user-attachments/assets/bf76bf29-b95b-4e11-a895-0fd325fcc8c7" />


### 2ã€Get_Pagesource:

è¿™ä¸ªè„šæœ¬çš„åŠŸèƒ½æ˜¯å±äºé€šè¿‡seleniumå»æ¨¡æ‹Ÿè¯·æ±‚åï¼Œç„¶åæŠŠå¯¹åº”çš„å‰ç«¯é¡µé¢æºç å†…å®¹æ–‡ä»¶è¿›è¡Œä¸‹è½½åˆ°æœ¬åœ°ï¼Œåªè¦åœ¨è„šæœ¬ä»£ç é‡Œé¢å®šä¹‰å¥½å¯¹åº”çš„ç›®æ ‡URLå³å¯è¿›è¡Œè‡ªåŠ¨çˆ¬å–ï¼Œå½“ç„¶éœ€è¦è®¾ç½®å¥½ä»£ç†è‡ªåŠ¨åˆ‡æ¢æ¨¡å¼ï¼Œé¿å…è¢«WAFå°IPã€‚

Usage: python3 Get_Pagesource.py


### 3ã€Download_har:

å› ä¸ºå¦‚æœå•çº¯çš„ä½¿ç”¨äº†Get_Pagesource.pyè„šæœ¬å»ä¸‹è½½å‰ç«¯é¡µé¢æºç å†…å®¹æ–‡ä»¶çš„è¯ï¼Œå¯èƒ½ä¼šå­˜åœ¨éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±ï¼Œå› ä¸ºç›®å‰å¾ˆå¤šç½‘ç«™ä¸ºäº†èƒ½æ–¹ä¾¿å¤§å®¶èƒ½å¤Ÿå¿«é€Ÿçš„è®¿é—®ï¼ŒåŸºæœ¬ä¸Šéƒ½æ˜¯ç”¨äº†å¼‚æ­¥åŠ è½½æ–¹å¼ï¼Œå› æ­¤åªæœ‰å½“ä½ åœ¨æµè§ˆçš„è¿‡ç¨‹ä¸­åˆ»æ„å»ç‚¹å‡»æŸä¸ªåŠŸèƒ½æˆ–è€…æŸä¸ªé¡µé¢çš„æ—¶å€™æ‰ä¼šåŠ è½½å¯¹åº”çš„èµ„æºæ–‡ä»¶ï¼Œè¿™ä¸ªåŠ è½½çš„æƒ…å†µå¯ä»¥åœ¨æµè§ˆå™¨çš„æ§åˆ¶å°çš„Networké€‰é¡¹å¡é¡µé¢æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼Œé‚£ä¹ˆåŒæ—¶ä¹Ÿå¯ä»¥æŠŠNetworké€‰é¡¹å¡çš„é¡µé¢å†…å®¹ã€å³å½“å‰å·²ç»å®ŒæˆåŠ è½½çš„èµ„æºæ–‡ä»¶ã€‘è¿›è¡Œä¸‹è½½åˆ°æœ¬åœ°ä¸ºharæ–‡ä»¶ï¼Œé‡Œé¢æœ‰æŒ‡å‘æ‰€æœ‰èµ„æºæ–‡ä»¶çš„URLåœ°å€ï¼Œé‚£ä¹ˆåªè¦æ‰§è¡Œè¿™ä¸ªDownload_har.pyè„šæœ¬å³å¯è‡ªåŠ¨å®Œæˆä¸‹è½½å¯¹åº”çš„èµ„æºæ–‡ä»¶ï¼Œå…¶å®Download_har.pyè„šæœ¬çš„è¯ç”Ÿæ˜¯ä¸ºäº†å¯¹ä¸Šé¢çš„Get_Pagesource.pyè„šæœ¬çš„ä¸€ä¸ªè¡¥å……ï¼Œå¯ä»¥å®Œæ•´çš„ä¸‹è½½ä¸€ä¸ªç›®æ ‡ç½‘ç«™çš„å‰ç«¯æºç ï¼Œç”šè‡³ä½ å¯ä»¥åœ¨æœ¬åœ°æ­å»ºå®ƒã€‚å½“ç„¶éœ€è¦è®¾ç½®å¥½ä»£ç†è‡ªåŠ¨åˆ‡æ¢æ¨¡å¼ï¼Œé¿å…è¢«WAFå°IPã€‚

Usage: python3 Download_har.py
